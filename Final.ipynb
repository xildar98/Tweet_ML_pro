{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LsM128lpXSJ",
        "colab_type": "code",
        "outputId": "2c6b3472-de5d-4bd4-8c48-47702c20a244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "plt.style.use('ggplot')\n",
        "stop=set(stopwords.words('english'))\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVWoy9fpX7P",
        "colab_type": "code",
        "outputId": "43c6e309-07fc-4ed2-8f6e-09a10d7c0dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "tweet= pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "tweet.head(3)\n",
        "\n",
        "def create_corpus(target):\n",
        "    corpus=[]\n",
        "    \n",
        "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
        "        for i in x:\n",
        "            corpus.append(i)\n",
        "    return corpus\n",
        "\n",
        "corpus=create_corpus(0)\n",
        "dic=defaultdict(int)\n",
        "for word in corpus:\n",
        "    if word in stop:\n",
        "        dic[word]+=1\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "\n",
        "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
        "df=pd.concat([tweet,test])\n",
        "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)\n",
        "\n",
        "remove_URL(example)\n",
        "df['text']=df['text'].apply(lambda x : remove_html(x))\n",
        "\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "\n",
        "df['text']=df['text'].apply(lambda x : remove_html(x))\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
        "\n",
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "example=\"I am a #king\"\n",
        "print(remove_punct(example))\n",
        "df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "        \n",
        "text = \"corect me plese\"\n",
        "correct_spellings(text)\n",
        "def create_corpus(df):\n",
        "    corpus=[]\n",
        "    for tweet in tqdm(df['text']):\n",
        "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
        "        corpus.append(words)\n",
        "    return corpus\n",
        "\n",
        "corpus=create_corpus(df)\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove\\.\\6B.zip\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am a king\n",
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10876/10876 [00:01<00:00, 6270.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-04-30 12:00:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-30 12:00:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-30 12:00:41--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.18MB/s    in 6m 26s  \n",
            "\n",
            "2020-04-30 12:07:07 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPDrlTpKyY1T",
        "colab_type": "code",
        "outputId": "6d2afbae-73dd-4ca3-e3e5-022b8433d8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "embedding_dict={}\n",
        "with open('glove.6B.100d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "MAX_LEN=50\n",
        "tokenizer_obj=Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(corpus)\n",
        "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
        "word_index=tokenizer_obj.word_index\n",
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,100))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "\n",
        "train=tweet_pad[:tweet.shape[0]]\n",
        "test=tweet_pad[tweet.shape[0]:]\n",
        "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
        "\n",
        "sample_sub=pd.read_csv('sample_submission.csv')\n",
        "\n",
        "\n",
        "\n",
        "bilstm=Sequential()\n",
        "\n",
        "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
        "                   input_length=MAX_LEN,trainable=False)\n",
        "\n",
        "bilstm.add(embedding)\n",
        "bilstm.add(SpatialDropout1D(0.2))\n",
        "bilstm.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
        "bilstm.add(Dense(64, activation='relu'))\n",
        "bilstm.add(Dense(1, activation='sigmoid'))\n",
        "optimzer=Adam()\n",
        "\n",
        "bilstm.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
        "\n",
        "optimzer=Adam(lr=1e-4)\n",
        "\n",
        "bilstm.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
        "history=bilstm.fit(X_train,y_train,batch_size=64,epochs=15,validation_data=(X_test,y_test),verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21444/21444 [00:00<00:00, 610919.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6471 samples, validate on 1142 samples\n",
            "Epoch 1/15\n",
            "6471/6471 [==============================] - 24s 4ms/step - loss: 0.6728 - accuracy: 0.5866 - val_loss: 0.6250 - val_accuracy: 0.6935\n",
            "Epoch 2/15\n",
            "6471/6471 [==============================] - 22s 3ms/step - loss: 0.5693 - accuracy: 0.7291 - val_loss: 0.4949 - val_accuracy: 0.7706\n",
            "Epoch 3/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.5223 - accuracy: 0.7594 - val_loss: 0.4757 - val_accuracy: 0.7811\n",
            "Epoch 4/15\n",
            "6471/6471 [==============================] - 22s 3ms/step - loss: 0.5044 - accuracy: 0.7646 - val_loss: 0.4659 - val_accuracy: 0.7881\n",
            "Epoch 5/15\n",
            "6471/6471 [==============================] - 22s 3ms/step - loss: 0.4894 - accuracy: 0.7750 - val_loss: 0.4570 - val_accuracy: 0.7960\n",
            "Epoch 6/15\n",
            "6471/6471 [==============================] - 22s 3ms/step - loss: 0.4876 - accuracy: 0.7741 - val_loss: 0.4510 - val_accuracy: 0.7960\n",
            "Epoch 7/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4766 - accuracy: 0.7835 - val_loss: 0.4483 - val_accuracy: 0.7986\n",
            "Epoch 8/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4698 - accuracy: 0.7870 - val_loss: 0.4461 - val_accuracy: 0.8004\n",
            "Epoch 9/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4649 - accuracy: 0.7891 - val_loss: 0.4485 - val_accuracy: 0.8021\n",
            "Epoch 10/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4668 - accuracy: 0.7903 - val_loss: 0.4458 - val_accuracy: 0.8004\n",
            "Epoch 11/15\n",
            "6471/6471 [==============================] - 22s 3ms/step - loss: 0.4642 - accuracy: 0.7900 - val_loss: 0.4422 - val_accuracy: 0.8065\n",
            "Epoch 12/15\n",
            "6471/6471 [==============================] - 22s 3ms/step - loss: 0.4634 - accuracy: 0.7937 - val_loss: 0.4413 - val_accuracy: 0.7986\n",
            "Epoch 13/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4545 - accuracy: 0.7969 - val_loss: 0.4437 - val_accuracy: 0.8056\n",
            "Epoch 14/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4568 - accuracy: 0.7911 - val_loss: 0.4415 - val_accuracy: 0.8047\n",
            "Epoch 15/15\n",
            "6471/6471 [==============================] - 21s 3ms/step - loss: 0.4565 - accuracy: 0.7929 - val_loss: 0.4383 - val_accuracy: 0.8065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtFznQ0Q3-ES",
        "colab_type": "code",
        "outputId": "6b60e700-fb63-4abb-f09d-de2fed43bdc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu6kHWM_wDjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD4wmtay4sos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuFx7G5v4KfM",
        "colab_type": "code",
        "outputId": "ab035cb4-0f9d-4153-8f4c-fc0595604722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(clf_output)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
        "trainb = pd.read_csv(\"train.csv\")\n",
        "testb = pd.read_csv(\"test.csv\")\n",
        "submission = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "train_input = bert_encode(trainb.text.values, tokenizer, max_len=160)\n",
        "test_input = bert_encode(testb.text.values, tokenizer, max_len=160)\n",
        "train_labels = trainb.target.values\n",
        "bert = build_model(bert_layer, max_len=160)\n",
        "bert.summary()\n",
        "train_history = bert.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=2,\n",
        "    batch_size=12\n",
        ")\n",
        "\n",
        "bert.save('bert.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "571/571 [==============================] - 451s 790ms/step - loss: 0.4580 - accuracy: 0.7933 - val_loss: 0.3742 - val_accuracy: 0.8373\n",
            "Epoch 2/2\n",
            "571/571 [==============================] - 448s 785ms/step - loss: 0.3357 - accuracy: 0.8593 - val_loss: 0.3983 - val_accuracy: 0.8268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irZUHYaI4IC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htu-qXNuhiOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "import numpy as np\n",
        "class Ensemble_models:\n",
        "  def __init__(self,n_models):\n",
        "    self.n_models = n_models\n",
        "    self.regressor = LogisticRegression()  \n",
        "  \n",
        "\n",
        "  def __extract_features__(self,predictions):\n",
        "    features = [predictions[i] for i in range(self.n_models)]\n",
        "    for i in range(self.n_models):\n",
        "      for j in range(self.n_models):\n",
        "        if i==j:\n",
        "          continue\n",
        "        features.append(predictions[i]*predictions[j])\n",
        "\n",
        "    return np.array(features).T.reshape(len(features[1]),-1)\n",
        "\n",
        "  def fit(self,predictions,ground_true):\n",
        "    features =self.__extract_features__(predictions)\n",
        "    print(features,ground_true)\n",
        "    self.regressor.fit(features, ground_true)\n",
        "\n",
        "  def predict(self,predictions):\n",
        "    features =self.__extract_features__(predictions)\n",
        "    print(self.regressor.coef_)\n",
        "    return self.regressor.predict(features)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSYLJy4bt3tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_kaggle(predictions):\n",
        "  y_pre=predictions\n",
        "  \n",
        "  y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
        "  sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
        "  sub.to_csv('submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVM0wEwiiX49",
        "colab_type": "code",
        "outputId": "e039d0dc-20b6-4b19-be5c-404e4db5f15f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "models = [bilstm,bert]\n",
        "bert_pred=bert.predict(train_input)\n",
        "bilst_pred =bilstm.predict(X_train)\n",
        "predictions = [bilst_pred,bert_pred[:len(bilst_pred)]]\n",
        "en_models = Ensemble_models(len(models))\n",
        "en_models.fit(predictions,y_train)\n",
        "answers = [bilstm.predict(test),bert.predict(test_input)]\n",
        "answer = en_models.predict(answers)\n",
        "predict_kaggle(answer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.14231974 0.97749925 0.13911743 0.13911743]\n",
            " [0.14903761 0.9830027  0.14650439 0.14650439]\n",
            " [0.10162592 0.95558417 0.09711212 0.09711212]\n",
            " ...\n",
            " [0.14860359 0.2894278  0.04301001 0.04301001]\n",
            " [0.9698935  0.10754597 0.10430814 0.10430814]\n",
            " [0.08722026 0.03001508 0.00261792 0.00261792]] [0 0 0 ... 0 1 0]\n",
            "[[5.31509387 0.10066894 0.0676214  0.0676214 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJE-OCXgFauo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiPgEgczmWli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlO5TEry2ihl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}